{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0617bb12",
   "metadata": {},
   "source": [
    "\n",
    "#### In this project, I will pre-process the text and train a text classifier using different feature representation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e4310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe78c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('FPB.csv',header = None,names = ['sentiments','headlines'],encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0a1781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiments</th>\n",
       "      <th>headlines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiments                                          headlines\n",
       "0    neutral  According to Gran , the company has no plans t...\n",
       "1    neutral  Technopolis plans to develop in stages an area...\n",
       "2   negative  The international electronic industry company ...\n",
       "3   positive  With the new production plant the company woul...\n",
       "4   positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea493053",
   "metadata": {},
   "source": [
    "### Part 1: Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24dfd190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # downloads a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a108c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93853a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token = []\n",
    "for i in range(len(data)):\n",
    "    sentence = re.sub(r'[^\\w\\s]', ' ', data.iloc[i,1]) #remove alphanumeric characters\n",
    "    token = word_tokenize(sentence)\n",
    "    word_token.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8664125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b09f6c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['According', 'to', 'Gran', 'the', 'company', 'has', 'no', 'plans', 'to', 'move', 'all', 'production', 'to', 'Russia', 'although', 'that', 'is', 'where', 'the', 'company', 'is', 'growing'], ['Technopolis', 'plans', 'to', 'develop', 'in', 'stages', 'an', 'area', 'of', 'no', 'less', 'than', '100', '000', 'square', 'meters', 'in', 'order', 'to', 'host', 'companies', 'working', 'in', 'computer', 'technologies', 'and', 'telecommunications', 'the', 'statement', 'said'], ['The', 'international', 'electronic', 'industry', 'company', 'Elcoteq', 'has', 'laid', 'off', 'tens', 'of', 'employees', 'from', 'its', 'Tallinn', 'facility', 'contrary', 'to', 'earlier', 'layoffs', 'the', 'company', 'contracted', 'the', 'ranks', 'of', 'its', 'office', 'workers', 'the', 'daily', 'Postimees', 'reported'], ['With', 'the', 'new', 'production', 'plant', 'the', 'company', 'would', 'increase', 'its', 'capacity', 'to', 'meet', 'the', 'expected', 'increase', 'in', 'demand', 'and', 'would', 'improve', 'the', 'use', 'of', 'raw', 'materials', 'and', 'therefore', 'increase', 'the', 'production', 'profitability'], ['According', 'to', 'the', 'company', 's', 'updated', 'strategy', 'for', 'the', 'years', '2009', '2012', 'Basware', 'targets', 'a', 'long', 'term', 'net', 'sales', 'growth', 'in', 'the', 'range', 'of', '20', '40', 'with', 'an', 'operating', 'profit', 'margin', 'of', '10', '20', 'of', 'net', 'sales']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#results of tokenization for the first 5 sentences \n",
    "word_token[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b83299",
   "metadata": {},
   "source": [
    "I used below function to process the text. I first removed alphanumeric characters, then I performed stemming of words based on PorterStemmer. I didn't remove stopwords as doing so will decrease the size of the corpus and the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54ab5f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#results of stemming for the first 5 sentences \n",
    "nltk.download('stopwords') # <--- this is new\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "ps = PorterStemmer() \n",
    "\n",
    "# return a list of tokens\n",
    "def pre_processing_by_nltk(doc, stemming = True, need_sent = False):\n",
    "    # step 1: get sentences \n",
    "    sentences = re.sub(r'[^\\w\\s]', ' ', doc)\n",
    "    sentences = sent_tokenize(sentences)\n",
    "    # step 2: get tokens\n",
    "    tokens = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        # step 3 (optional): stemming\n",
    "        if stemming:\n",
    "            words = [ps.stem(word) for word in words]\n",
    "        if need_sent:\n",
    "            tokens.append(words)\n",
    "        else:\n",
    "            tokens += words\n",
    "    return [w.lower() for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "445a9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_list = []\n",
    "for i in range(len(data)):\n",
    "    token = pre_processing_by_nltk(data.iloc[i,1])\n",
    "    stem_list.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa390db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['accord', 'to', 'gran', 'the', 'compani', 'ha', 'no', 'plan', 'to', 'move', 'all', 'product', 'to', 'russia', 'although', 'that', 'is', 'where', 'the', 'compani', 'is', 'grow'], ['technopoli', 'plan', 'to', 'develop', 'in', 'stage', 'an', 'area', 'of', 'no', 'less', 'than', '100', '000', 'squar', 'meter', 'in', 'order', 'to', 'host', 'compani', 'work', 'in', 'comput', 'technolog', 'and', 'telecommun', 'the', 'statement', 'said'], ['the', 'intern', 'electron', 'industri', 'compani', 'elcoteq', 'ha', 'laid', 'off', 'ten', 'of', 'employe', 'from', 'it', 'tallinn', 'facil', 'contrari', 'to', 'earlier', 'layoff', 'the', 'compani', 'contract', 'the', 'rank', 'of', 'it', 'offic', 'worker', 'the', 'daili', 'postime', 'report'], ['with', 'the', 'new', 'product', 'plant', 'the', 'compani', 'would', 'increas', 'it', 'capac', 'to', 'meet', 'the', 'expect', 'increas', 'in', 'demand', 'and', 'would', 'improv', 'the', 'use', 'of', 'raw', 'materi', 'and', 'therefor', 'increas', 'the', 'product', 'profit'], ['accord', 'to', 'the', 'compani', 's', 'updat', 'strategi', 'for', 'the', 'year', '2009', '2012', 'baswar', 'target', 'a', 'long', 'term', 'net', 'sale', 'growth', 'in', 'the', 'rang', 'of', '20', '40', 'with', 'an', 'oper', 'profit', 'margin', 'of', '10', '20', 'of', 'net', 'sale']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#results of tokenization&stemming for the first 5 sentences \n",
    "stem_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14871d6",
   "metadata": {},
   "source": [
    "### Part 2: Bag Of Words (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c3309f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test and train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting the train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.headlines, data.sentiments, random_state=42, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b37ecd",
   "metadata": {},
   "source": [
    "In the following part, I created a word frequency dictionary based on the train dataset. Keys of the freq are words that contained in the train dataset, and values associated with each key are times that word appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22d27f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "freq = defaultdict(int)\n",
    "\n",
    "\n",
    "corpus = ' '.join(list(X_train))\n",
    "new_corpus = re.sub(r'[^\\w\\s]', ' ', corpus)\n",
    "raw_tokens = new_corpus.lower().split()\n",
    "raw_tokens = [ps.stem(word) for word in raw_tokens]\n",
    "raw_tokens_stop = [w.lower() for w in raw_tokens if w.lower() not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e3ad3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82506\n",
      "55014\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_tokens))\n",
    "print(len(raw_tokens_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cf82bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary that contains frequency of a certain word\n",
    "for token in raw_tokens:\n",
    "    freq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16fed709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "IDF, vocab = dict(), dict()\n",
    "for token in freq:\n",
    "    vocab[token] = len(vocab) #create a fix index of all words\n",
    "    IDF[token] = log(1 + len(X_train) / freq[token]) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee0e2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF['<UNK>'] = 1\n",
    "vocab['<UNK>'] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c88d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d67cce",
   "metadata": {},
   "source": [
    "### Train the classifier using method 1\n",
    "In this method, the feature is represented as a binary-valued vector of dimension equal to the size of the vocabulary. The value at an index is 1 if the word corresponding to that index is present in the document, else 0.\n",
    "When I first train the model, I received a warning indicating the model doesn't converge. Therefore I increased the number of iteration to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17264d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabEXIST(doc,index,freqdic):\n",
    "    tokens = pre_processing_by_nltk(doc)\n",
    "    x= []\n",
    "    for vob in index:\n",
    "        if vob not in tokens:\n",
    "            x.append(0)\n",
    "        else:\n",
    "            x.append(1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9561de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = []\n",
    "X_test_1 = []\n",
    "for doc in X_train: #create a feature vector \n",
    "    X_train_1.append(vocabEXIST(doc, index_list, freq))\n",
    "for doc in X_test:\n",
    "    X_test_1.append(vocabEXIST(doc, index_list, freq))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab1f62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "M1 = LogisticRegression(random_state=0,max_iter=1000).fit(X_train_1,y_train)\n",
    "predict_y1 = M1.predict(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "775325c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC is 0.89 , macro-f1 score is 0.72 , micro-f1 score 0.77\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "macro_f1_1 = sklearn.metrics.f1_score(y_test, predict_y1,average='macro')\n",
    "micro_f1_1 = sklearn.metrics.f1_score(y_test, predict_y1,average='micro')\n",
    "y_predict_prob = M1.predict_proba(X_test_1)\n",
    "auc_1 = sklearn.metrics.roc_auc_score(y_test,y_predict_prob,multi_class = 'ovr')\n",
    "\n",
    "print('AUROC is',round(auc_1,2),', macro-f1 score is', round(macro_f1_1,2),', micro-f1 score',round(micro_f1_1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438ee29",
   "metadata": {},
   "source": [
    "### Train the classifier using method 2\n",
    "In this method, the feature is represented by a vector of dimension equal to the size of the vocabulary where the value corresponding to each word is its frequency in the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec0d1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabfreq(doc,index,freqdic):\n",
    "    tokens = pre_processing_by_nltk(doc)\n",
    "    x= []\n",
    "    for vob in index:\n",
    "        if vob not in tokens:\n",
    "            x.append(0)\n",
    "        else:\n",
    "            x.append(freqdic[vob])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c62cbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = []\n",
    "X_test_2 = []\n",
    "for doc in X_train:\n",
    "    X_train_2.append(vocabfreq(doc, index_list, freq))\n",
    "for doc in X_test:\n",
    "    X_test_2.append(vocabfreq(doc, index_list, freq))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69831cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "M2 = LogisticRegression(random_state=0,max_iter=2000).fit(X_train_2,y_train)\n",
    "predict_y2 = M2.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1622bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC is 0.86  macro-f1 score is 0.69 micro-f1 score 0.74\n"
     ]
    }
   ],
   "source": [
    "macro_f1_2 = sklearn.metrics.f1_score(y_test, predict_y2,average='macro')\n",
    "micro_f1_2 = sklearn.metrics.f1_score(y_test, predict_y2,average='micro')\n",
    "y_predict_prob = M2.predict_proba(X_test_2)\n",
    "auc_2 = sklearn.metrics.roc_auc_score(y_test,y_predict_prob,multi_class = 'ovr')\n",
    "\n",
    "print('AUROC is',round(auc_2,2),' macro-f1 score is', round(macro_f1_2,2),'micro-f1 score',round(micro_f1_2,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111cfe8e",
   "metadata": {},
   "source": [
    "### Train the classifier using method 3\n",
    "In this method, the feature is represented by a vector of dimension equal to the size of the vocabulary where the value corresponding to each word is its tf-idf value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a2b5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 3\n",
    "def tfidf_feature_extractor(doc, vocab, IDF):\n",
    "    tokens = pre_processing_by_nltk(doc)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in vocab:\n",
    "            tokens[i] = '<UNK>'\n",
    "    TF = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        TF[token] += 1\n",
    "    x = [0] * len(vocab)\n",
    "    for token in set(tokens):\n",
    "        tfidf = log(TF[token] + 1) * IDF[token]\n",
    "        token_id = vocab[token]\n",
    "#         print(token, TF[token], IDF[token])\n",
    "        x[token_id] = tfidf # this will be a dense matrix\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a7be18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = []\n",
    "X_test_3 = []\n",
    "\n",
    "for doc in X_train:\n",
    "    X_train_3.append(tfidf_feature_extractor(doc, vocab, IDF))\n",
    "\n",
    "for doc in X_test:\n",
    "    X_test_3.append(tfidf_feature_extractor(doc, vocab, IDF))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d92a29cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "M3 = LogisticRegression(random_state=0).fit(X_train_3,y_train)\n",
    "predict_y3 = M3.predict(X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2973aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC is 0.87  macro-f1 score is 0.72 micro-f1 score 0.76\n"
     ]
    }
   ],
   "source": [
    "macro_f1_3 = sklearn.metrics.f1_score(y_test, predict_y3,average='macro')\n",
    "micro_f1_3 = sklearn.metrics.f1_score(y_test, predict_y3,average='micro')\n",
    "y_predict_prob = M3.predict_proba(X_test_3)\n",
    "auc_3 = sklearn.metrics.roc_auc_score(y_test,y_predict_prob,multi_class = 'ovr')\n",
    "\n",
    "print('AUROC is',round(auc_3,2),' macro-f1 score is', round(macro_f1_3,2),'micro-f1 score',round(micro_f1_3,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a2cddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = [auc_1,auc_2,auc_3]\n",
    "mi_f1 = [micro_f1_1,micro_f1_2,micro_f1_3]\n",
    "ma_f1 = [macro_f1_1,macro_f1_2,macro_f1_3]\n",
    "sum_table = pd.DataFrame().assign(auc = auc,macro_f1 =ma_f1 ,micro_f1 =mi_f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f8beb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_table.index = ['Binary','Frequency','TF-IDF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1eaab814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>micro_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.894445</td>\n",
       "      <td>0.724276</td>\n",
       "      <td>0.768041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frequency</th>\n",
       "      <td>0.862403</td>\n",
       "      <td>0.691797</td>\n",
       "      <td>0.738144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.872490</td>\n",
       "      <td>0.723114</td>\n",
       "      <td>0.758763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                auc  macro_f1  micro_f1\n",
       "Binary     0.894445  0.724276  0.768041\n",
       "Frequency  0.862403  0.691797  0.738144\n",
       "TF-IDF     0.872490  0.723114  0.758763"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08172d1",
   "metadata": {},
   "source": [
    "For this specific text classifier, I will choose the method 1 since it has the highest AUC, macro-f1 score, and micro-f1 score. The first model dominates the other 2 based on the three metric we choose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
