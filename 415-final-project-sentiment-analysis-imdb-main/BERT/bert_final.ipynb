{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2 (3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RUDVcP-N517W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49dc4879-0038-46c8-e8cd-d302012cf790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 26.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U tensorflow-text --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official"
      ],
      "metadata": {
        "id": "D8ymeT4J53fp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f696ec96-bb1d-46c3-f11a-76a7d272dfaa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.2 MB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 39.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 37.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 6.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 47.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 234 kB 48.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 45 kB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 39.7 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "0hDHNrjA54uH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        "# keras uitlity to download a file\n",
        "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url, untar=True, cache_dir='.', cache_subdir='')"
      ],
      "metadata": {
        "id": "_RZ4eR_g561e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ae7816-cdde-425b-f0be-75015afdd461"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 8s 0us/step\n",
            "84140032/84125825 [==============================] - 8s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = '/content/aclImdb'\n",
        "\n",
        "train_dir = os.path.join(dataset_dir,'train')"
      ],
      "metadata": {
        "id": "8Hmf0i3t59au"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove unused folders to make it easier to load the data\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "metadata": {
        "id": "CmydkF-M5_1h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32\n",
        "seed = 123\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "class_names = raw_train_ds.class_names\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=batch_size)\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O22kKmkX6RG1",
        "outputId": "cfbc5d6d-ab1a-4b3b-ad46-29e32e0bfe20"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(f'Review: {text_batch.numpy()[i]}')\n",
        "    label = label_batch.numpy()[i]\n",
        "    print(f'Label : {label} ({class_names[label]})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3ImaMmH6bfa",
        "outputId": "5e74f771-73d7-4fe0-e2dc-5ce29397a849"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: b'After, I watched the films... I thought, \"Why the heck was this film such a high success in the Korean Box Office?\" Even thought the movie had a clever/unusal scenario, the acting wasn\\'t that good and the characters weren\\'t very interesting. For a Korean movie... I liked the fighting scenes. If you want to watch a film without thinking, this is the film for you. But I got to admit... the film was kind of childish... 6/10'\n",
            "Label : 1 (pos)\n",
            "Review: b'Or released on DVD or screened on a cable channel like Amer. Life TV network. I have been watching another favorite, \"Voyage to the Bottom of the Sea\", as well as \"Lost in Space\" and Land of Giants\". They\\'ve been showing them forever but aren\\'t receptive to suggestions for other shows. My father and I were big fans as I was already a big science/electronics nut, (still am) and my father was an old school chum of Nader. They both attended Oxy together. I still have memories of several of the episodes even though I was only 9. More so than any show that old. I think it was televised on Sat. after \"Bonanza\". Some of the episodes I recall are the one where he takes the experimental drug that slows down action. Or the one where he body surfs the big ones, (I did that too!) Or the one where there was a mine cave in and he conveys how to use mind control to have the trapped people slow their breathing by entering a trance-like state. That is the one show that I wish I could see again. I got my wish with the original \"Outer Limits\" and \"Sci-Fi Theater...John'\n",
            "Label : 1 (pos)\n",
            "Review: b\"I saw Insomniac's Nightmare not to long ago for the first time and I have to say, I really found it to be quite good. If you are a fan of Dominic Monaghan you will love it. The hole movie takes place inside his mind -or does it? The acting from everyone else is a little rushed and shaky and some of the scenes could be cut down but it works out in the end. The extras on the DVD are just as great as the film, if not greater for those Dom fans. It has tons of candid moments from the set, outtakes and a great interview with the director. Anyone who has gone through making an independent film will love to watch Tess (the director), Dom and everyone else on the very small close personal set try to bang out this little trippy creepy film. It was pretty enjoyable and I'm glad to have it in my collection.\"\n",
            "Label : 1 (pos)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm8sVgBMmG12",
        "outputId": "2a40dc6f-1789-4479-c6f4-7939bf0c6a45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "metadata": {
        "id": "AuKl1PlCng4W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test = ['this is such an amazing movie!. I hate the movie', 'hello world']\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGW0m_U7ng6w",
        "outputId": "0d7b2a2a-cf75-4e6c-c2d3-4ee7aaa5fa9d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys       : ['input_word_ids', 'input_type_ids', 'input_mask']\n",
            "Shape      : (2, 128)\n",
            "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999 1012 1045 5223 1996]\n",
            "Input Mask : [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Type Ids   : [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "metadata": {
        "id": "ULHQA7UZng9F"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF6dR0j6n_aY",
        "outputId": "aaf98826-755f-4caf-e2a4-82ad7cee7dcf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded BERT: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\n",
            "Pooled Outputs Shape:(2, 768)\n",
            "Pooled Outputs Values:[-0.85526216 -0.33712423 -0.34576908  0.57520497  0.38547078 -0.08428411\n",
            "  0.44962656  0.20554389 -0.5116271  -0.9999461  -0.00613103  0.83263266]\n",
            "Sequence Outputs Shape:(2, 128, 768)\n",
            "Sequence Outputs Values:[[ 0.38575327 -0.11281332  0.2981122  ... -0.1316025   0.8360076\n",
            "   0.28022325]\n",
            " [-0.50844646 -0.32244614  0.19049025 ... -0.6035149   1.2848651\n",
            "   0.24631244]\n",
            " [-0.08842065 -0.00249235  0.41535258 ... -0.39508688  0.7541475\n",
            "   0.4246072 ]\n",
            " ...\n",
            " [ 0.4423539   0.48887545  0.11532195 ... -0.28149486  0.48708117\n",
            "   0.48195764]\n",
            " [ 0.39349335  0.22193849  0.32810032 ...  0.5328984   0.90055084\n",
            "   0.42265135]\n",
            " [-0.02805402 -0.54921985  0.04732138 ... -0.12337839  1.4534825\n",
            "  -0.01174106]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "XUlRERZ8n_cz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = build_classifier_model()\n",
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "print(tf.sigmoid(bert_raw_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF9IMhdyn_fL",
        "outputId": "37f9769a-87e3-48a4-d0e1-c0cce0ca7b67"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0.19168532]\n",
            " [0.2257794 ]], shape=(2, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "metadata": {
        "id": "YhrWeHPKn_hZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "metadata": {
        "id": "JlismRqpn_jp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "metadata": {
        "id": "-6nEkJZknhAM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=train_ds,\n",
        "                               validation_data=val_ds,\n",
        "                               epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6DdiV24nhB5",
        "outputId": "4e338ec5-18aa-419e-bbee-3077acca5e90"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\n",
            "Epoch 1/3\n",
            "625/625 [==============================] - 1196s 2s/step - loss: 0.3845 - binary_accuracy: 0.8099 - val_loss: 0.2766 - val_binary_accuracy: 0.8716\n",
            "Epoch 2/3\n",
            "625/625 [==============================] - 1170s 2s/step - loss: 0.2093 - binary_accuracy: 0.9150 - val_loss: 0.3525 - val_binary_accuracy: 0.8694\n",
            "Epoch 3/3\n",
            "625/625 [==============================] - 1171s 2s/step - loss: 0.1156 - binary_accuracy: 0.9593 - val_loss: 0.4198 - val_binary_accuracy: 0.8840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = classifier_model.evaluate(test_ds)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSb8WyY8pHMg",
        "outputId": "87cde8bd-f55a-42a8-8518-58d31928b1b0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 514s 657ms/step - loss: 0.4080 - binary_accuracy: 0.8866\n",
            "Loss: 0.40803593397140503\n",
            "Accuracy: 0.8866000175476074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_name = 'imdb'\n",
        "# saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
        "\n",
        "# classifier_model.save(saved_model_path, include_optimizer=False)"
      ],
      "metadata": {
        "id": "sY82BHYWEK_G"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reloaded_model = tf.saved_model.load(saved_model_path)"
      ],
      "metadata": {
        "id": "8cWWoggaELmR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "Jo002Lq8kg7X"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "for text_batch, label_batch in test_ds.take(782):\n",
        "    text.append(text_batch.numpy())"
      ],
      "metadata": {
        "id": "BBLaGTbdD8AT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = []\n",
        "for i in range(782):\n",
        "  for x in text[i].tolist():\n",
        "    review.append(x)"
      ],
      "metadata": {
        "id": "ULveLk1mkS3G"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier_model.predict(stringlist[:100])"
      ],
      "metadata": {
        "id": "uj684KOQ4yAF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stringlist = [x.decode('utf-8') for x in review]"
      ],
      "metadata": {
        "id": "1iNoa3Bl4tfW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_raw_result = classifier_model(tf.constant(stringlist))"
      ],
      "metadata": {
        "id": "u5AhXMCq4uG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = tf.sigmoid(bert_raw_result)"
      ],
      "metadata": {
        "id": "aXu0Hu4j_mPt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test"
      ],
      "metadata": {
        "id": "LsmgjRZvYQrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f2e93f-8f42-4b91-b3aa-0e2f6135825b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100, 1), dtype=float32, numpy=\n",
              "array([[1.14429346e-03],\n",
              "       [6.56694756e-04],\n",
              "       [9.64505017e-01],\n",
              "       [6.16917200e-02],\n",
              "       [7.83895492e-04],\n",
              "       [9.76338685e-01],\n",
              "       [2.57240189e-03],\n",
              "       [7.67604681e-04],\n",
              "       [9.70268309e-01],\n",
              "       [1.17672002e-03],\n",
              "       [1.51271722e-03],\n",
              "       [9.97238398e-01],\n",
              "       [8.13097417e-01],\n",
              "       [8.09938821e-04],\n",
              "       [5.88641107e-01],\n",
              "       [9.97752726e-01],\n",
              "       [4.03841678e-03],\n",
              "       [2.53060320e-03],\n",
              "       [2.42060772e-03],\n",
              "       [9.90192755e-04],\n",
              "       [9.01901687e-04],\n",
              "       [9.96664464e-01],\n",
              "       [2.10419875e-02],\n",
              "       [9.97775018e-01],\n",
              "       [1.68832198e-01],\n",
              "       [9.97576296e-01],\n",
              "       [9.84956324e-01],\n",
              "       [9.97829378e-01],\n",
              "       [1.20407995e-03],\n",
              "       [9.97815490e-01],\n",
              "       [9.57358420e-01],\n",
              "       [9.71617294e-04],\n",
              "       [8.81352127e-01],\n",
              "       [9.97318566e-01],\n",
              "       [9.96941864e-01],\n",
              "       [4.82522309e-01],\n",
              "       [9.95937943e-01],\n",
              "       [5.76440915e-02],\n",
              "       [8.33368511e-04],\n",
              "       [9.97970045e-01],\n",
              "       [7.10035034e-04],\n",
              "       [9.95949984e-01],\n",
              "       [1.79999657e-02],\n",
              "       [1.08870119e-03],\n",
              "       [9.97442007e-01],\n",
              "       [1.26933465e-02],\n",
              "       [9.53314546e-03],\n",
              "       [9.92752075e-01],\n",
              "       [1.51325334e-02],\n",
              "       [8.43762994e-01],\n",
              "       [1.86339971e-02],\n",
              "       [9.97074842e-01],\n",
              "       [1.77391633e-01],\n",
              "       [9.96835649e-01],\n",
              "       [9.96537447e-01],\n",
              "       [9.94522333e-01],\n",
              "       [9.96057272e-01],\n",
              "       [8.43967427e-04],\n",
              "       [4.55387160e-02],\n",
              "       [2.26795767e-03],\n",
              "       [7.52850901e-04],\n",
              "       [9.64557290e-01],\n",
              "       [6.65668689e-04],\n",
              "       [7.44777024e-01],\n",
              "       [1.08784274e-03],\n",
              "       [9.92766857e-01],\n",
              "       [4.89326790e-02],\n",
              "       [6.75202929e-04],\n",
              "       [9.98197496e-01],\n",
              "       [1.07287907e-03],\n",
              "       [6.50980603e-03],\n",
              "       [6.08373026e-04],\n",
              "       [9.81932402e-01],\n",
              "       [1.65501423e-03],\n",
              "       [1.37166050e-03],\n",
              "       [5.60206294e-01],\n",
              "       [1.03356270e-02],\n",
              "       [5.30149508e-03],\n",
              "       [2.71636061e-03],\n",
              "       [9.97992039e-01],\n",
              "       [2.02981010e-03],\n",
              "       [9.94535685e-01],\n",
              "       [9.95462477e-01],\n",
              "       [9.97296274e-01],\n",
              "       [9.97402966e-01],\n",
              "       [9.96674776e-01],\n",
              "       [7.74662418e-04],\n",
              "       [9.96489286e-01],\n",
              "       [9.95106816e-01],\n",
              "       [8.33423634e-04],\n",
              "       [9.93636370e-01],\n",
              "       [4.25780416e-01],\n",
              "       [9.96689141e-01],\n",
              "       [7.46033562e-04],\n",
              "       [3.34235397e-03],\n",
              "       [9.51743277e-04],\n",
              "       [1.07792090e-03],\n",
              "       [1.67700127e-01],\n",
              "       [3.40634316e-01],\n",
              "       [9.97080028e-01]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = []\n",
        "for text_batch, label_batch in test_ds.take(782):\n",
        "    label.append(label_batch.numpy())"
      ],
      "metadata": {
        "id": "U26o4NCEjOcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real = []\n",
        "for i in range(782):\n",
        "  for x in label[i].tolist():\n",
        "    real.append(x)"
      ],
      "metadata": {
        "id": "WVM0I9MCjHW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\"review\": review, \"pred\": pred_test})"
      ],
      "metadata": {
        "id": "HEF3y-KHkYOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "df.to_csv('output.csv', encoding = 'utf-8-sig') \n",
        "files.download('output.csv')"
      ],
      "metadata": {
        "id": "RI4uO5SNkaSU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}