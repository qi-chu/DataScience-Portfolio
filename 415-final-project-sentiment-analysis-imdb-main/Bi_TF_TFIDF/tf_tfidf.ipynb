{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf469399-b77a-4008-a97c-d63e43079c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "effa3d46-0466-4991-b0f5-852e731a97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3a33c96-37e3-4792-bb79-6b20e0ccc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total = pd.read_csv('train_processed.csv')\n",
    "test_total = pd.read_csv('test_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa5d063c-506e-4a84-8460-387efb4bdb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # downloads a model\n",
    "nltk.download('stopwords') # <--- this is new\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "ps = PorterStemmer() \n",
    "\n",
    "# return a list of tokens\n",
    "def pre_processing_by_nltk(doc, stemming = True, need_sent = False):\n",
    "    # step 1: get sentences \n",
    "    #sentences = re.sub(r'[^\\w\\s]', ' ', doc)\n",
    "    #sentences = sent_tokenize(sentences)\n",
    "    # step 2: get tokens\n",
    "    #tokens = []\n",
    "    #for sent in sentences:\n",
    "    words = word_tokenize(doc)\n",
    "        # step 3 (optional): stemming\n",
    "    if stemming:\n",
    "        words = [ps.stem(word) for word in words]\n",
    "#     if need_sent:\n",
    "#         tokens.append(words)\n",
    "#     else:\n",
    "#         tokens += words\n",
    "    return [w.lower() for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b736b0a6-9dd8-4abb-8c88-1b3e32b4dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [word_tokenize(_d) for i, _d in enumerate(train_total[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "862d59d8-3a09-4c36-b171-24936fc96108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "freq = defaultdict(int)\n",
    "for doc in tagged_data:\n",
    "    for token in doc:\n",
    "        freq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7253808-fbd1-4224-be75-23d4ee71e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_copy = freq.copy()\n",
    "for key, value in freq_copy.items():\n",
    "    if value < 30 or value > 25000 :\n",
    "        del freq[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd60e5ee-a921-4083-b646-a1663a647c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "IDF, vocab = dict(), dict()\n",
    "for token in freq:\n",
    "    vocab[token] = len(vocab) #create a fix index of all words\n",
    "    IDF[token] = log(1 + len(train_total) / freq[token]) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61261d81-045f-4154-8b67-3da738a99192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabFreq(doc,freqdic):\n",
    "    tokens = doc.split(' ')\n",
    "    #tokens = pre_processing_by_nltk(doc)\n",
    "    x= [0]*7911 #vector size equal to\n",
    "    cor = list(freqdic.keys())\n",
    "    for i in range(len(freqdic.keys())):\n",
    "        if cor[i] in tokens:\n",
    "            x[i] = freqdic[cor[i]]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81f14b07-a2ff-4cbd-ad5a-a52fcbd9508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total = train_total.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43a53897-077f-4643-8c5b-bc0fb01f5e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "X_train_2 = []\n",
    "for doc in train_total['text']: #create a feature vector \n",
    "    X_train_2.append(vocabFreq(doc, freq))\n",
    "    if len(X_train_2) == 100:\n",
    "        print(100)\n",
    "    if len(X_train_2) == 1000:\n",
    "        print(1000)\n",
    "    if len(X_train_2) == 10000:\n",
    "        print(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e51dd5bf-0c05-49ca-bf97-f8840704b465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.5min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "M2 = LogisticRegression(random_state=0,max_iter=10000, C = 1,n_jobs = -1,verbose = 1).fit(X_train_2,train_total['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef69eea5-88f7-444a-86e5-6751f2908a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "X_test_2 = []\n",
    "for doc in test_total['text']: #create a feature vector \n",
    "    X_test_2.append(vocabFreq(doc, freq))\n",
    "    if len(X_test_2) == 100:\n",
    "        print(100)\n",
    "    if len(X_test_2) == 1000:\n",
    "        print(1000)\n",
    "    if len(X_test_2) == 10000:\n",
    "        print(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b88bcdf5-524c-4d81-bec5-b244b429fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y2 = M2.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8793348-2b0d-4815-87e7-82a8012db068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro F1 is  0.822838047250092 micr F1 is  0.82284\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "macro_f1_2 = sklearn.metrics.f1_score(test_total['label'], predict_y2,average='macro')\n",
    "micro_f1_2 = sklearn.metrics.f1_score(test_total['label'], predict_y2,average='micro')\n",
    "print('macro F1 is ',macro_f1_2, 'micr F1 is ',micro_f1_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5764664-ba28-4989-862d-ca31cd9dd053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9002813952"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc2 = sklearn.metrics.roc_auc_score(test_total['label'], M2.predict_proba(X_test_2)[:, 1])\n",
    "auc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da9069b6-04b4-4526-892a-8e7137696f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_result = pd.DataFrame(\n",
    "    {'label': test_total['label'],\n",
    "     'predict': M2.predict_proba(X_test_2)[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "affc6d3c-c574-4c7f-ab17-4ff6e2551dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_result.to_csv('df_tf_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8510fd01-f01f-4d8c-853f-8a546e7cf5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "np.set_printoptions(precision=2)\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "\n",
    "tfidf.fit(X_train_2)\n",
    "Xtrain_TFIDF = tfidf.transform(X_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac729bb5-0eea-43b5-aa35-64386a3875e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_feature_extractor(doc, vocab, IDF):\n",
    "    tokens = doc.split(' ')\n",
    "    x = [0]*7911\n",
    "    for token in set(tokens):\n",
    "        if token in freq.keys():\n",
    "            tfidf = freq[token] * IDF[token]\n",
    "            token_id = vocab[token]\n",
    "            x[token_id] = tfidf # this will be a dense matrix\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1b1c383-57e8-4b53-9b8a-81293dc8fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = []\n",
    "X_test_3 = []\n",
    "\n",
    "for doc in train_total['text']:\n",
    "    X_train_3.append(tfidf_feature_extractor(doc, vocab, IDF))\n",
    "\n",
    "for doc in test_total['text']:\n",
    "    X_test_3.append(tfidf_feature_extractor(doc, vocab, IDF))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1a4b67c0-01cc-4538-9d06-93769f789539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.6min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "M3 = LogisticRegression(random_state=0,max_iter=1e7, C = 5,verbose = 1e-3).fit(X_train_3,train_total['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "60d0730b-a431-471d-a64a-5420ed2d2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y3 = M3.predict(Xtest_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d93aa3c8-1ad8-4161-b67d-10fec2290245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro F1 is  0.7656016772483888 micr F1 is  0.77252\n"
     ]
    }
   ],
   "source": [
    "macro_f1_3 = sklearn.metrics.f1_score(test_total['label'], predict_y3,average='macro')\n",
    "micro_f1_3 = sklearn.metrics.f1_score(test_total['label'], predict_y3,average='micro')\n",
    "print('macro F1 is ',macro_f1_3, 'micr F1 is ',micro_f1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d0ae235a-e411-410c-8dad-c81ab587cc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8957381056"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc2 = sklearn.metrics.roc_auc_score(test_total['label'], M3.predict_proba(Xtest_TFIDF)[:, 1])\n",
    "auc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89200ee9-067c-4e8f-b744-f29691517bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_result = pd.DataFrame(\n",
    "    {'label': test_total['label'],\n",
    "     'predict': M3.predict_proba(Xtest_TFIDF)[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "028a21d8-faf1-422f-87bb-126a90fbd087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_result.to_csv('df_tfidf_result.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
